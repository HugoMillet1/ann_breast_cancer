{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Biologically Inspired Computation - Coursework 1\n",
        "Team members :\n",
        "Hugo Millet\n",
        "Timothe Petitjean"
      ],
      "metadata": {
        "id": "ZszYA7rGGO4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1 : Imports**"
      ],
      "metadata": {
        "id": "pG3yaJMKGdAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UUXLlSNhPkoj"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy import *\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 : Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "2UDuRO6vGhj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# By using excel, we changed the original format of the data into a csv file, for more simplicity\n",
        "dataset = pd.read_csv('breast_cancer.csv')"
      ],
      "metadata": {
        "id": "tU7Y5dr7aCrO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip7pEAQKC5x0",
        "outputId": "6330a403-72b0-4534-e46f-5e8a306422e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKRKNFXNC8ls",
        "outputId": "12585cf1-b24a-4269-bc6a-61671474e464"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 32 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Column1   569 non-null    int64  \n",
            " 1   Column2   569 non-null    object \n",
            " 2   Column3   569 non-null    float64\n",
            " 3   Column4   569 non-null    float64\n",
            " 4   Column5   569 non-null    float64\n",
            " 5   Column6   569 non-null    float64\n",
            " 6   Column7   569 non-null    float64\n",
            " 7   Column8   569 non-null    float64\n",
            " 8   Column9   569 non-null    float64\n",
            " 9   Column10  569 non-null    float64\n",
            " 10  Column11  569 non-null    float64\n",
            " 11  Column12  569 non-null    float64\n",
            " 12  Column13  569 non-null    float64\n",
            " 13  Column14  569 non-null    float64\n",
            " 14  Column15  569 non-null    float64\n",
            " 15  Column16  569 non-null    float64\n",
            " 16  Column17  569 non-null    float64\n",
            " 17  Column18  569 non-null    float64\n",
            " 18  Column19  569 non-null    float64\n",
            " 19  Column20  569 non-null    float64\n",
            " 20  Column21  569 non-null    float64\n",
            " 21  Column22  569 non-null    float64\n",
            " 22  Column23  569 non-null    float64\n",
            " 23  Column24  569 non-null    float64\n",
            " 24  Column25  569 non-null    float64\n",
            " 25  Column26  569 non-null    float64\n",
            " 26  Column27  569 non-null    float64\n",
            " 27  Column28  569 non-null    float64\n",
            " 28  Column29  569 non-null    float64\n",
            " 29  Column30  569 non-null    float64\n",
            " 30  Column31  569 non-null    float64\n",
            " 31  Column32  569 non-null    float64\n",
            "dtypes: float64(30), int64(1), object(1)\n",
            "memory usage: 142.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "5A972FeRDAmC",
        "outputId": "a81cf2f8-f5bf-43f8-b2f8-873ae3fb2f6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Column1 Column2  Column3  Column4  Column5  Column6  Column7  Column8  \\\n",
              "0    842302       M    17.99    10.38   122.80   1001.0  0.11840  0.27760   \n",
              "1    842517       M    20.57    17.77   132.90   1326.0  0.08474  0.07864   \n",
              "2  84300903       M    19.69    21.25   130.00   1203.0  0.10960  0.15990   \n",
              "3  84348301       M    11.42    20.38    77.58    386.1  0.14250  0.28390   \n",
              "4  84358402       M    20.29    14.34   135.10   1297.0  0.10030  0.13280   \n",
              "\n",
              "   Column9  Column10  ...  Column23  Column24  Column25  Column26  Column27  \\\n",
              "0   0.3001   0.14710  ...     25.38     17.33    184.60    2019.0    0.1622   \n",
              "1   0.0869   0.07017  ...     24.99     23.41    158.80    1956.0    0.1238   \n",
              "2   0.1974   0.12790  ...     23.57     25.53    152.50    1709.0    0.1444   \n",
              "3   0.2414   0.10520  ...     14.91     26.50     98.87     567.7    0.2098   \n",
              "4   0.1980   0.10430  ...     22.54     16.67    152.20    1575.0    0.1374   \n",
              "\n",
              "   Column28  Column29  Column30  Column31  Column32  \n",
              "0    0.6656    0.7119    0.2654    0.4601   0.11890  \n",
              "1    0.1866    0.2416    0.1860    0.2750   0.08902  \n",
              "2    0.4245    0.4504    0.2430    0.3613   0.08758  \n",
              "3    0.8663    0.6869    0.2575    0.6638   0.17300  \n",
              "4    0.2050    0.4000    0.1625    0.2364   0.07678  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6f28cc4f-e379-4f12-9a56-6a94727fee6a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column1</th>\n",
              "      <th>Column2</th>\n",
              "      <th>Column3</th>\n",
              "      <th>Column4</th>\n",
              "      <th>Column5</th>\n",
              "      <th>Column6</th>\n",
              "      <th>Column7</th>\n",
              "      <th>Column8</th>\n",
              "      <th>Column9</th>\n",
              "      <th>Column10</th>\n",
              "      <th>...</th>\n",
              "      <th>Column23</th>\n",
              "      <th>Column24</th>\n",
              "      <th>Column25</th>\n",
              "      <th>Column26</th>\n",
              "      <th>Column27</th>\n",
              "      <th>Column28</th>\n",
              "      <th>Column29</th>\n",
              "      <th>Column30</th>\n",
              "      <th>Column31</th>\n",
              "      <th>Column32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f28cc4f-e379-4f12-9a56-6a94727fee6a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6f28cc4f-e379-4f12-9a56-6a94727fee6a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6f28cc4f-e379-4f12-9a56-6a94727fee6a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3 : Features Remodeling**"
      ],
      "metadata": {
        "id": "WfRhkxhpGl5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.pop(\"Column1\") # The ID is a useless feature\n",
        "dataset = dataset.rename(columns = {\"Column2\":\"Diagnostic\"}) # Naming the output class correctly\n",
        "for i in range(29):\n",
        "  dataset = dataset.rename(columns = {\"Column\"+str(i+3):\"Feature\"+str(i)}) # More clarity for the other features\n",
        "dataset = dataset.rename(columns = {\"Column32\":\"Feature29\"})"
      ],
      "metadata": {
        "id": "ldjFgFEPaCvm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['Diagnostic'] = dataset['Diagnostic'].apply(lambda x : 1 if x == \"M\" else 0) # From a letter to 0 or 1"
      ],
      "metadata": {
        "id": "7V6-QYZcaC2u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ZwVsi-7BDD-K",
        "outputId": "7712cb09-f14a-4216-8a80-a91e153ba793"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Diagnostic  Feature0  Feature1  Feature2  Feature3  Feature4  Feature5  \\\n",
              "0           1     17.99     10.38    122.80    1001.0   0.11840   0.27760   \n",
              "1           1     20.57     17.77    132.90    1326.0   0.08474   0.07864   \n",
              "2           1     19.69     21.25    130.00    1203.0   0.10960   0.15990   \n",
              "3           1     11.42     20.38     77.58     386.1   0.14250   0.28390   \n",
              "4           1     20.29     14.34    135.10    1297.0   0.10030   0.13280   \n",
              "\n",
              "   Feature6  Feature7  Feature8  ...  Feature20  Feature21  Feature22  \\\n",
              "0    0.3001   0.14710    0.2419  ...      25.38      17.33     184.60   \n",
              "1    0.0869   0.07017    0.1812  ...      24.99      23.41     158.80   \n",
              "2    0.1974   0.12790    0.2069  ...      23.57      25.53     152.50   \n",
              "3    0.2414   0.10520    0.2597  ...      14.91      26.50      98.87   \n",
              "4    0.1980   0.10430    0.1809  ...      22.54      16.67     152.20   \n",
              "\n",
              "   Feature23  Feature24  Feature25  Feature26  Feature27  Feature28  Feature29  \n",
              "0     2019.0     0.1622     0.6656     0.7119     0.2654     0.4601    0.11890  \n",
              "1     1956.0     0.1238     0.1866     0.2416     0.1860     0.2750    0.08902  \n",
              "2     1709.0     0.1444     0.4245     0.4504     0.2430     0.3613    0.08758  \n",
              "3      567.7     0.2098     0.8663     0.6869     0.2575     0.6638    0.17300  \n",
              "4     1575.0     0.1374     0.2050     0.4000     0.1625     0.2364    0.07678  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d99ae76-5fc6-4d7f-9480-816d0266439e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Diagnostic</th>\n",
              "      <th>Feature0</th>\n",
              "      <th>Feature1</th>\n",
              "      <th>Feature2</th>\n",
              "      <th>Feature3</th>\n",
              "      <th>Feature4</th>\n",
              "      <th>Feature5</th>\n",
              "      <th>Feature6</th>\n",
              "      <th>Feature7</th>\n",
              "      <th>Feature8</th>\n",
              "      <th>...</th>\n",
              "      <th>Feature20</th>\n",
              "      <th>Feature21</th>\n",
              "      <th>Feature22</th>\n",
              "      <th>Feature23</th>\n",
              "      <th>Feature24</th>\n",
              "      <th>Feature25</th>\n",
              "      <th>Feature26</th>\n",
              "      <th>Feature27</th>\n",
              "      <th>Feature28</th>\n",
              "      <th>Feature29</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d99ae76-5fc6-4d7f-9480-816d0266439e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d99ae76-5fc6-4d7f-9480-816d0266439e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d99ae76-5fc6-4d7f-9480-816d0266439e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_class = dataset.drop('Diagnostic', 1)\n",
        "y_class = dataset['Diagnostic']\n",
        "# Creating training and testing sub datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_class, y_class, test_size=0.2, random_state=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_HFQnkAaN2R",
        "outputId": "b7f860e3-a6ef-4214-fdcb-7fc8b7e1ca64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4 : ANN Building**"
      ],
      "metadata": {
        "id": "fH4dL1bFGsUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "# Scaling the values of the features\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n"
      ],
      "metadata": {
        "id": "FNv_yjrZaRtp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with numpy is more convenient\n",
        "# IMPORTANT : AFTER RUNNING ONCE, COMMENT THESE TWO LINES\n",
        "# Otherwise the program will try to convert something already in a numpy format in a numpy and there will be\n",
        "# an error\n",
        "y_train = y_train.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ],
      "metadata": {
        "id": "jOlAT8eUbnA7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    res = 1 / (1 + math.exp(-x*0.05)) # 0.05 for a smoother function according to our input values\n",
        "    return res\n",
        "\n",
        "def relu(x):\n",
        "  res = max(0.0, x)\n",
        "  return res\n",
        "\n",
        "def hyperbolicTan(x):\n",
        "  res = np.tanh(x)\n",
        "  return res"
      ],
      "metadata": {
        "id": "Mdoto3DcaVIa"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorized versions of our activations functions\n",
        "sigm_vect = np.vectorize(sigmoid)\n",
        "relu_vect = np.vectorize(relu)\n",
        "tanh_vect = np.vectorize(hyperbolicTan)"
      ],
      "metadata": {
        "id": "p_UwXy6NaVRC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ann(nbneurons):\n",
        "  # We have only one hidden layer, but we can choose its number of neurons\n",
        "  global weights1, weights2,bias1, bias2\n",
        "  random.seed()\n",
        "  input_layer = np.random.random((1, len(X_train[0]))) # shape : 1xn_features\n",
        "  hidden_layer1 = np.random.random((1, nbneurons)) # shape : 1xn_neurons\n",
        "  weights1 = np.random.random((len(X_train[1]), hidden_layer1.shape[1])) # shape : n_features*n_neurons\n",
        "  bias1 = np.random.random((1, hidden_layer1.shape[1])) # shape : 1xn_neurons\n",
        "  weights2 = np.random.random((nbneurons, 1)) # shape : n_neuronsx1\n",
        "  bias2 = 0 # shape : 1x1"
      ],
      "metadata": {
        "id": "snXQ2tJraBgm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 5 : ANN Training**"
      ],
      "metadata": {
        "id": "u4KnX0eNJ64e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ann_train(activ_funct, loss_funct, learning_rate, n_epochs):\n",
        "  global weights1, weights2,bias1, bias2,dweights1\n",
        "  for j in range(n_epochs):\n",
        "    print(\"-epoch \" + str(j) + \"/\" + str(n_epochs) + \"-\")\n",
        "    for i in range(X_train.shape[0]):\n",
        "      print(\"iteration :\" + str(i))\n",
        "      ## Defining input layer\n",
        "      input_layer = np.zeros((1, len(X_train[i])))\n",
        "      input_layer[0,:] = X_train[i] # Input being the features of the example\n",
        "      ## Forward propagation\n",
        "      print(\"---FORWARD PART---\")\n",
        "      # Hidden layer\n",
        "      hidden_layer1 = np.matmul(input_layer,weights1)+bias1 # h = W*i+b\n",
        "      # Different activations functions\n",
        "      if activ_funct == \"relu\":\n",
        "        hidden_layer1 = relu_vect(hidden_layer1)\n",
        "      elif activ_funct == \"tanh\":\n",
        "        hidden_layer1 = tanh_vect(hidden_layer1)\n",
        "      else:\n",
        "        hidden_layer1 = sigm_vect(hidden_layer1)\n",
        "\n",
        "      # Output layer\n",
        "      output = np.matmul(hidden_layer1, weights2)+bias2 # o = W*h+b\n",
        "      # Different activations functions\n",
        "      if activ_funct == \"relu\":\n",
        "        output = relu_vect(output)\n",
        "      elif activ_funct == \"tanh\":\n",
        "        output = tanh_vect(output)\n",
        "      else:\n",
        "        output = sigm_vect(output)\n",
        "\n",
        "      ## Loss calculation\n",
        "      if loss_funct == \"cross_entropy\":\n",
        "        loss = -(y_train[i]*math.log(output)+(1-y_train[i])*math.log(1-output))\n",
        "      elif loss_funct == \"hinge_loss\":\n",
        "        loss = max(0, 1-y_train[i]*output)\n",
        "      loss_list[i] = loss\n",
        "      print(\"loss : \" + str(loss))\n",
        "\n",
        "      ## Backward propagation\n",
        "      print(\"---BACKWARD PART---\")\n",
        "      # Output -> Hidden layer\n",
        "      # Derivatives of different loss functions\n",
        "      if loss_funct == \"cross_entropy\":\n",
        "        dloss = -y_train[i]/output + (1-y_train[i])/(1-output)\n",
        "      elif loss_funct == \"hinge_loss\":\n",
        "        if loss == 0:\n",
        "          dloss = 0\n",
        "        else:\n",
        "          dloss = -y_train[i]\n",
        "      if activ_funct == \"relu\":\n",
        "        if output == 0:\n",
        "          drelu = 0\n",
        "        else:\n",
        "          drelu = 1\n",
        "        dz = dloss*drelu\n",
        "      elif activ_funct == \"tanh\":\n",
        "        dz = dloss*(1-output**2)\n",
        "      else:\n",
        "        dz = output-y_train[i]\n",
        "\n",
        "      dweights2 = np.zeros(weights2.shape)\n",
        "      # Derivatives of weights = input*dz\n",
        "      dweights2 = hidden_layer1[0].reshape(weights2.shape)*dz\n",
        "\n",
        "      # Derivatives of bias = dz\n",
        "      dbias2 = dz\n",
        "\n",
        "      alpha = learning_rate\n",
        "\n",
        "      # Updating the weights\n",
        "      weights2 = weights2-alpha*dweights2\n",
        "      bias2 = bias2-alpha*dbias2\n",
        "\n",
        "      # Hidden -> Input layer\n",
        "      if activ_funct == \"relu\":\n",
        "        dreluh = 0\n",
        "        for h in hidden_layer1[0]:\n",
        "          if h > 0:\n",
        "            dreluh = 1\n",
        "          # We need a 1x1 value in order for our dz matrix to be the right dimension\n",
        "          # But this method is probably not the right one since our accuracy is not satisfying\n",
        "        dz = dloss*np.matmul(weights2, dreluh)\n",
        "      elif activ_funct == \"tanh\":\n",
        "        dtanh = 1-np.matmul(hidden_layer1, np.transpose(hidden_layer1))\n",
        "        dz = dloss*np.matmul(weights2, dtanh)\n",
        "      else:\n",
        "        dsigm = np.matmul(hidden_layer1, 1-np.transpose(hidden_layer1))\n",
        "        dz = output-y_train[i]*(np.matmul(weights2, dsigm))\n",
        "      \n",
        "      dz = np.transpose(dz)\n",
        "      dweights1 = np.zeros(weights1.shape)\n",
        "      dbias1 = np.zeros(bias1.shape)\n",
        "      # Derivatives of weights = input*dz\n",
        "      dweights1 = np.matmul(np.transpose(input_layer), dz)\n",
        "      dbias1 = dz\n",
        "      \n",
        "      # Updating weights and bias\n",
        "      weights1 = weights1-alpha*dweights1\n",
        "      bias1 = bias1-alpha*dbias1"
      ],
      "metadata": {
        "id": "mHO-gnK8aZgc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 6 : ANN Testing**"
      ],
      "metadata": {
        "id": "j8dTWpBiKC9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ann_test(activ_funct):\n",
        "    n_good_class = 0 # number of examples classed successfully\n",
        "    global weights1, weights2,bias1, bias2,dweights1\n",
        "    for i in range(X_test.shape[0]):\n",
        "      # Same program as the forward propagation for the training, but this time on the test sample\n",
        "      print(\"iteration :\" + str(i))\n",
        "      ## Defining input layer\n",
        "      input_layer = np.zeros((1, len(X_test[i])))\n",
        "      input_layer[0,:] = X_test[i]\n",
        "      ## Forward propagation\n",
        "      print(\"---FORWARD PART---\")\n",
        "      # Hidden layer\n",
        "      hidden_layer1 = np.matmul(input_layer,weights1)+bias1\n",
        "      if activ_funct == \"relu\":\n",
        "        hidden_layer1 = relu_vect(hidden_layer1)\n",
        "      elif activ_funct == \"tanh\":\n",
        "        hidden_layer1 = tanh_vect(hidden_layer1)\n",
        "      else:\n",
        "        hidden_layer1 = sigm_vect(hidden_layer1)\n",
        "      # Output layer\n",
        "      output = np.matmul(hidden_layer1, weights2)+bias2\n",
        "      if activ_funct == \"relu\":\n",
        "        output = relu_vect(output)\n",
        "      elif activ_funct == \"tanh\":\n",
        "        output = tanh_vect(output)\n",
        "      else:\n",
        "        output = sigm_vect(output)\n",
        "      # Binary classification\n",
        "      if output < 0.5:\n",
        "        output = 0\n",
        "      else:\n",
        "        output = 1\n",
        "      if output == y_test[i]:\n",
        "        print(\"example successfully classed\")\n",
        "        n_good_class += 1\n",
        "    accuracy = n_good_class/len(y_test)\n",
        "    print(\"accuracy : \" + str(accuracy))"
      ],
      "metadata": {
        "id": "u_zZnIbLbWiO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = np.zeros(X_train.shape[0])"
      ],
      "metadata": {
        "id": "luMGUtqMbwAk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_ann(4) # hidden layer with n neurons"
      ],
      "metadata": {
        "id": "Wt-EXpIdbay2"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_train(\"sigmoid\", \"cross_entropy\", 0.5, 1) # Training the network with the hyperparameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZszFEnMbdOs",
        "outputId": "83b49572-4e6c-457b-ea03-87d820f15ccf"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-epoch 0/1-\n",
            "iteration :0\n",
            "---FORWARD PART---\n",
            "loss : 0.6586998750373233\n",
            "---BACKWARD PART---\n",
            "iteration :1\n",
            "---FORWARD PART---\n",
            "loss : 0.6279844466390367\n",
            "---BACKWARD PART---\n",
            "iteration :2\n",
            "---FORWARD PART---\n",
            "loss : 0.6355493528334878\n",
            "---BACKWARD PART---\n",
            "iteration :3\n",
            "---FORWARD PART---\n",
            "loss : 0.7544272383426655\n",
            "---BACKWARD PART---\n",
            "iteration :4\n",
            "---FORWARD PART---\n",
            "loss : 0.6207113722344249\n",
            "---BACKWARD PART---\n",
            "iteration :5\n",
            "---FORWARD PART---\n",
            "loss : 0.5986095832485487\n",
            "---BACKWARD PART---\n",
            "iteration :6\n",
            "---FORWARD PART---\n",
            "loss : 0.7711396545677064\n",
            "---BACKWARD PART---\n",
            "iteration :7\n",
            "---FORWARD PART---\n",
            "loss : 0.7682860651676914\n",
            "---BACKWARD PART---\n",
            "iteration :8\n",
            "---FORWARD PART---\n",
            "loss : 0.7310729366091322\n",
            "---BACKWARD PART---\n",
            "iteration :9\n",
            "---FORWARD PART---\n",
            "loss : 0.7467443099040026\n",
            "---BACKWARD PART---\n",
            "iteration :10\n",
            "---FORWARD PART---\n",
            "loss : 0.7134523219458906\n",
            "---BACKWARD PART---\n",
            "iteration :11\n",
            "---FORWARD PART---\n",
            "loss : 0.6884037449461416\n",
            "---BACKWARD PART---\n",
            "iteration :12\n",
            "---FORWARD PART---\n",
            "loss : 0.6869726047616885\n",
            "---BACKWARD PART---\n",
            "iteration :13\n",
            "---FORWARD PART---\n",
            "loss : 0.6785680632678653\n",
            "---BACKWARD PART---\n",
            "iteration :14\n",
            "---FORWARD PART---\n",
            "loss : 0.6914886449216648\n",
            "---BACKWARD PART---\n",
            "iteration :15\n",
            "---FORWARD PART---\n",
            "loss : 0.6580299024754299\n",
            "---BACKWARD PART---\n",
            "iteration :16\n",
            "---FORWARD PART---\n",
            "loss : 0.7089840439543816\n",
            "---BACKWARD PART---\n",
            "iteration :17\n",
            "---FORWARD PART---\n",
            "loss : 0.6599439875123018\n",
            "---BACKWARD PART---\n",
            "iteration :18\n",
            "---FORWARD PART---\n",
            "loss : 0.6596623240607535\n",
            "---BACKWARD PART---\n",
            "iteration :19\n",
            "---FORWARD PART---\n",
            "loss : 0.6597149738307811\n",
            "---BACKWARD PART---\n",
            "iteration :20\n",
            "---FORWARD PART---\n",
            "loss : 0.6411443814289536\n",
            "---BACKWARD PART---\n",
            "iteration :21\n",
            "---FORWARD PART---\n",
            "loss : 0.7339999658688893\n",
            "---BACKWARD PART---\n",
            "iteration :22\n",
            "---FORWARD PART---\n",
            "loss : 0.649614709167933\n",
            "---BACKWARD PART---\n",
            "iteration :23\n",
            "---FORWARD PART---\n",
            "loss : 0.6375431396407054\n",
            "---BACKWARD PART---\n",
            "iteration :24\n",
            "---FORWARD PART---\n",
            "loss : 0.7277054793640378\n",
            "---BACKWARD PART---\n",
            "iteration :25\n",
            "---FORWARD PART---\n",
            "loss : 0.6989145012477755\n",
            "---BACKWARD PART---\n",
            "iteration :26\n",
            "---FORWARD PART---\n",
            "loss : 0.6845298591677966\n",
            "---BACKWARD PART---\n",
            "iteration :27\n",
            "---FORWARD PART---\n",
            "loss : 0.6538956087255727\n",
            "---BACKWARD PART---\n",
            "iteration :28\n",
            "---FORWARD PART---\n",
            "loss : 0.6531478717607604\n",
            "---BACKWARD PART---\n",
            "iteration :29\n",
            "---FORWARD PART---\n",
            "loss : 0.6556305901251835\n",
            "---BACKWARD PART---\n",
            "iteration :30\n",
            "---FORWARD PART---\n",
            "loss : 0.6459967194011992\n",
            "---BACKWARD PART---\n",
            "iteration :31\n",
            "---FORWARD PART---\n",
            "loss : 0.7164246542891954\n",
            "---BACKWARD PART---\n",
            "iteration :32\n",
            "---FORWARD PART---\n",
            "loss : 0.668861648419313\n",
            "---BACKWARD PART---\n",
            "iteration :33\n",
            "---FORWARD PART---\n",
            "loss : 0.6704657739508683\n",
            "---BACKWARD PART---\n",
            "iteration :34\n",
            "---FORWARD PART---\n",
            "loss : 0.6679134447383471\n",
            "---BACKWARD PART---\n",
            "iteration :35\n",
            "---FORWARD PART---\n",
            "loss : 0.6261945608592355\n",
            "---BACKWARD PART---\n",
            "iteration :36\n",
            "---FORWARD PART---\n",
            "loss : 0.6189491136731805\n",
            "---BACKWARD PART---\n",
            "iteration :37\n",
            "---FORWARD PART---\n",
            "loss : 0.7064298641554478\n",
            "---BACKWARD PART---\n",
            "iteration :38\n",
            "---FORWARD PART---\n",
            "loss : 0.674797354279465\n",
            "---BACKWARD PART---\n",
            "iteration :39\n",
            "---FORWARD PART---\n",
            "loss : 0.6508171584896233\n",
            "---BACKWARD PART---\n",
            "iteration :40\n",
            "---FORWARD PART---\n",
            "loss : 0.625504071288472\n",
            "---BACKWARD PART---\n",
            "iteration :41\n",
            "---FORWARD PART---\n",
            "loss : 0.629160827076504\n",
            "---BACKWARD PART---\n",
            "iteration :42\n",
            "---FORWARD PART---\n",
            "loss : 0.6465207625738962\n",
            "---BACKWARD PART---\n",
            "iteration :43\n",
            "---FORWARD PART---\n",
            "loss : 0.6246731494938249\n",
            "---BACKWARD PART---\n",
            "iteration :44\n",
            "---FORWARD PART---\n",
            "loss : 0.6153342073888864\n",
            "---BACKWARD PART---\n",
            "iteration :45\n",
            "---FORWARD PART---\n",
            "loss : 0.6404551651284008\n",
            "---BACKWARD PART---\n",
            "iteration :46\n",
            "---FORWARD PART---\n",
            "loss : 0.6304462606565174\n",
            "---BACKWARD PART---\n",
            "iteration :47\n",
            "---FORWARD PART---\n",
            "loss : 0.5911846948748636\n",
            "---BACKWARD PART---\n",
            "iteration :48\n",
            "---FORWARD PART---\n",
            "loss : 0.6247111204647667\n",
            "---BACKWARD PART---\n",
            "iteration :49\n",
            "---FORWARD PART---\n",
            "loss : 0.6190169426771287\n",
            "---BACKWARD PART---\n",
            "iteration :50\n",
            "---FORWARD PART---\n",
            "loss : 0.6000433295741082\n",
            "---BACKWARD PART---\n",
            "iteration :51\n",
            "---FORWARD PART---\n",
            "loss : 0.6038758855373725\n",
            "---BACKWARD PART---\n",
            "iteration :52\n",
            "---FORWARD PART---\n",
            "loss : 0.6286280083226861\n",
            "---BACKWARD PART---\n",
            "iteration :53\n",
            "---FORWARD PART---\n",
            "loss : 0.5780122820942132\n",
            "---BACKWARD PART---\n",
            "iteration :54\n",
            "---FORWARD PART---\n",
            "loss : 0.6425889027244293\n",
            "---BACKWARD PART---\n",
            "iteration :55\n",
            "---FORWARD PART---\n",
            "loss : 0.5758445373411495\n",
            "---BACKWARD PART---\n",
            "iteration :56\n",
            "---FORWARD PART---\n",
            "loss : 0.567739578846994\n",
            "---BACKWARD PART---\n",
            "iteration :57\n",
            "---FORWARD PART---\n",
            "loss : 0.543761545095016\n",
            "---BACKWARD PART---\n",
            "iteration :58\n",
            "---FORWARD PART---\n",
            "loss : 0.6202797981090445\n",
            "---BACKWARD PART---\n",
            "iteration :59\n",
            "---FORWARD PART---\n",
            "loss : 0.5530289879843242\n",
            "---BACKWARD PART---\n",
            "iteration :60\n",
            "---FORWARD PART---\n",
            "loss : 0.5712538070285247\n",
            "---BACKWARD PART---\n",
            "iteration :61\n",
            "---FORWARD PART---\n",
            "loss : 0.5611785649131172\n",
            "---BACKWARD PART---\n",
            "iteration :62\n",
            "---FORWARD PART---\n",
            "loss : 0.6194027126648355\n",
            "---BACKWARD PART---\n",
            "iteration :63\n",
            "---FORWARD PART---\n",
            "loss : 0.5911053601193238\n",
            "---BACKWARD PART---\n",
            "iteration :64\n",
            "---FORWARD PART---\n",
            "loss : 0.5705890692488593\n",
            "---BACKWARD PART---\n",
            "iteration :65\n",
            "---FORWARD PART---\n",
            "loss : 0.5473615659267319\n",
            "---BACKWARD PART---\n",
            "iteration :66\n",
            "---FORWARD PART---\n",
            "loss : 0.5330602627168625\n",
            "---BACKWARD PART---\n",
            "iteration :67\n",
            "---FORWARD PART---\n",
            "loss : 0.5303423643149533\n",
            "---BACKWARD PART---\n",
            "iteration :68\n",
            "---FORWARD PART---\n",
            "loss : 0.6566088662946528\n",
            "---BACKWARD PART---\n",
            "iteration :69\n",
            "---FORWARD PART---\n",
            "loss : 0.5052731540549968\n",
            "---BACKWARD PART---\n",
            "iteration :70\n",
            "---FORWARD PART---\n",
            "loss : 0.5060271297382788\n",
            "---BACKWARD PART---\n",
            "iteration :71\n",
            "---FORWARD PART---\n",
            "loss : 0.5934207020062137\n",
            "---BACKWARD PART---\n",
            "iteration :72\n",
            "---FORWARD PART---\n",
            "loss : 0.5081619386988774\n",
            "---BACKWARD PART---\n",
            "iteration :73\n",
            "---FORWARD PART---\n",
            "loss : 0.6101929412985373\n",
            "---BACKWARD PART---\n",
            "iteration :74\n",
            "---FORWARD PART---\n",
            "loss : 0.5583846725900036\n",
            "---BACKWARD PART---\n",
            "iteration :75\n",
            "---FORWARD PART---\n",
            "loss : 0.5502710514938255\n",
            "---BACKWARD PART---\n",
            "iteration :76\n",
            "---FORWARD PART---\n",
            "loss : 0.5221027375603505\n",
            "---BACKWARD PART---\n",
            "iteration :77\n",
            "---FORWARD PART---\n",
            "loss : 0.5768546758248753\n",
            "---BACKWARD PART---\n",
            "iteration :78\n",
            "---FORWARD PART---\n",
            "loss : 0.6258406335643479\n",
            "---BACKWARD PART---\n",
            "iteration :79\n",
            "---FORWARD PART---\n",
            "loss : 0.5111387746270003\n",
            "---BACKWARD PART---\n",
            "iteration :80\n",
            "---FORWARD PART---\n",
            "loss : 0.4818416938524265\n",
            "---BACKWARD PART---\n",
            "iteration :81\n",
            "---FORWARD PART---\n",
            "loss : 0.6174885928851841\n",
            "---BACKWARD PART---\n",
            "iteration :82\n",
            "---FORWARD PART---\n",
            "loss : 0.5181106505080962\n",
            "---BACKWARD PART---\n",
            "iteration :83\n",
            "---FORWARD PART---\n",
            "loss : 0.49372010414960876\n",
            "---BACKWARD PART---\n",
            "iteration :84\n",
            "---FORWARD PART---\n",
            "loss : 0.5119896559073808\n",
            "---BACKWARD PART---\n",
            "iteration :85\n",
            "---FORWARD PART---\n",
            "loss : 0.474981799328495\n",
            "---BACKWARD PART---\n",
            "iteration :86\n",
            "---FORWARD PART---\n",
            "loss : 0.5648255984539804\n",
            "---BACKWARD PART---\n",
            "iteration :87\n",
            "---FORWARD PART---\n",
            "loss : 0.8556854062758852\n",
            "---BACKWARD PART---\n",
            "iteration :88\n",
            "---FORWARD PART---\n",
            "loss : 0.5637875933073232\n",
            "---BACKWARD PART---\n",
            "iteration :89\n",
            "---FORWARD PART---\n",
            "loss : 0.6166282426461904\n",
            "---BACKWARD PART---\n",
            "iteration :90\n",
            "---FORWARD PART---\n",
            "loss : 0.5356523465402673\n",
            "---BACKWARD PART---\n",
            "iteration :91\n",
            "---FORWARD PART---\n",
            "loss : 0.45513590813900395\n",
            "---BACKWARD PART---\n",
            "iteration :92\n",
            "---FORWARD PART---\n",
            "loss : 0.4497068570440018\n",
            "---BACKWARD PART---\n",
            "iteration :93\n",
            "---FORWARD PART---\n",
            "loss : 0.5481176723360108\n",
            "---BACKWARD PART---\n",
            "iteration :94\n",
            "---FORWARD PART---\n",
            "loss : 0.5416873728342231\n",
            "---BACKWARD PART---\n",
            "iteration :95\n",
            "---FORWARD PART---\n",
            "loss : 0.44864976436532217\n",
            "---BACKWARD PART---\n",
            "iteration :96\n",
            "---FORWARD PART---\n",
            "loss : 0.6494472657513907\n",
            "---BACKWARD PART---\n",
            "iteration :97\n",
            "---FORWARD PART---\n",
            "loss : 0.48992349504091115\n",
            "---BACKWARD PART---\n",
            "iteration :98\n",
            "---FORWARD PART---\n",
            "loss : 0.5685716779357572\n",
            "---BACKWARD PART---\n",
            "iteration :99\n",
            "---FORWARD PART---\n",
            "loss : 0.48296138288855484\n",
            "---BACKWARD PART---\n",
            "iteration :100\n",
            "---FORWARD PART---\n",
            "loss : 0.5940707467032473\n",
            "---BACKWARD PART---\n",
            "iteration :101\n",
            "---FORWARD PART---\n",
            "loss : 0.45049651682407094\n",
            "---BACKWARD PART---\n",
            "iteration :102\n",
            "---FORWARD PART---\n",
            "loss : 0.4451073797474921\n",
            "---BACKWARD PART---\n",
            "iteration :103\n",
            "---FORWARD PART---\n",
            "loss : 0.4357223681890182\n",
            "---BACKWARD PART---\n",
            "iteration :104\n",
            "---FORWARD PART---\n",
            "loss : 0.4341952182388956\n",
            "---BACKWARD PART---\n",
            "iteration :105\n",
            "---FORWARD PART---\n",
            "loss : 0.46950579515652546\n",
            "---BACKWARD PART---\n",
            "iteration :106\n",
            "---FORWARD PART---\n",
            "loss : 0.4749101864675845\n",
            "---BACKWARD PART---\n",
            "iteration :107\n",
            "---FORWARD PART---\n",
            "loss : 0.42953331258710936\n",
            "---BACKWARD PART---\n",
            "iteration :108\n",
            "---FORWARD PART---\n",
            "loss : 0.42362595756020116\n",
            "---BACKWARD PART---\n",
            "iteration :109\n",
            "---FORWARD PART---\n",
            "loss : 0.4164376856135486\n",
            "---BACKWARD PART---\n",
            "iteration :110\n",
            "---FORWARD PART---\n",
            "loss : 0.42002817513796037\n",
            "---BACKWARD PART---\n",
            "iteration :111\n",
            "---FORWARD PART---\n",
            "loss : 0.5240208936759687\n",
            "---BACKWARD PART---\n",
            "iteration :112\n",
            "---FORWARD PART---\n",
            "loss : 0.4895753697477262\n",
            "---BACKWARD PART---\n",
            "iteration :113\n",
            "---FORWARD PART---\n",
            "loss : 0.4326020968241894\n",
            "---BACKWARD PART---\n",
            "iteration :114\n",
            "---FORWARD PART---\n",
            "loss : 0.4573107917366666\n",
            "---BACKWARD PART---\n",
            "iteration :115\n",
            "---FORWARD PART---\n",
            "loss : 0.4161569509621976\n",
            "---BACKWARD PART---\n",
            "iteration :116\n",
            "---FORWARD PART---\n",
            "loss : 0.41848916019792554\n",
            "---BACKWARD PART---\n",
            "iteration :117\n",
            "---FORWARD PART---\n",
            "loss : 0.7006594925116638\n",
            "---BACKWARD PART---\n",
            "iteration :118\n",
            "---FORWARD PART---\n",
            "loss : 0.6291021068489672\n",
            "---BACKWARD PART---\n",
            "iteration :119\n",
            "---FORWARD PART---\n",
            "loss : 0.42010928590087165\n",
            "---BACKWARD PART---\n",
            "iteration :120\n",
            "---FORWARD PART---\n",
            "loss : 0.41272863120071807\n",
            "---BACKWARD PART---\n",
            "iteration :121\n",
            "---FORWARD PART---\n",
            "loss : 0.4262660080077674\n",
            "---BACKWARD PART---\n",
            "iteration :122\n",
            "---FORWARD PART---\n",
            "loss : 0.42544015315116385\n",
            "---BACKWARD PART---\n",
            "iteration :123\n",
            "---FORWARD PART---\n",
            "loss : 0.39484311378867726\n",
            "---BACKWARD PART---\n",
            "iteration :124\n",
            "---FORWARD PART---\n",
            "loss : 0.4145321475856055\n",
            "---BACKWARD PART---\n",
            "iteration :125\n",
            "---FORWARD PART---\n",
            "loss : 0.41570524303092765\n",
            "---BACKWARD PART---\n",
            "iteration :126\n",
            "---FORWARD PART---\n",
            "loss : 0.39641078685682135\n",
            "---BACKWARD PART---\n",
            "iteration :127\n",
            "---FORWARD PART---\n",
            "loss : 0.3946220916931985\n",
            "---BACKWARD PART---\n",
            "iteration :128\n",
            "---FORWARD PART---\n",
            "loss : 0.46826564608960064\n",
            "---BACKWARD PART---\n",
            "iteration :129\n",
            "---FORWARD PART---\n",
            "loss : 0.6649482165341258\n",
            "---BACKWARD PART---\n",
            "iteration :130\n",
            "---FORWARD PART---\n",
            "loss : 0.43409042779468543\n",
            "---BACKWARD PART---\n",
            "iteration :131\n",
            "---FORWARD PART---\n",
            "loss : 0.3695443162972212\n",
            "---BACKWARD PART---\n",
            "iteration :132\n",
            "---FORWARD PART---\n",
            "loss : 0.5362837446564188\n",
            "---BACKWARD PART---\n",
            "iteration :133\n",
            "---FORWARD PART---\n",
            "loss : 0.43592003044244776\n",
            "---BACKWARD PART---\n",
            "iteration :134\n",
            "---FORWARD PART---\n",
            "loss : 0.38171270713686084\n",
            "---BACKWARD PART---\n",
            "iteration :135\n",
            "---FORWARD PART---\n",
            "loss : 0.9056206395278585\n",
            "---BACKWARD PART---\n",
            "iteration :136\n",
            "---FORWARD PART---\n",
            "loss : 0.4468289941932218\n",
            "---BACKWARD PART---\n",
            "iteration :137\n",
            "---FORWARD PART---\n",
            "loss : 0.42897317503155064\n",
            "---BACKWARD PART---\n",
            "iteration :138\n",
            "---FORWARD PART---\n",
            "loss : 0.35672770878928295\n",
            "---BACKWARD PART---\n",
            "iteration :139\n",
            "---FORWARD PART---\n",
            "loss : 0.4193324757697109\n",
            "---BACKWARD PART---\n",
            "iteration :140\n",
            "---FORWARD PART---\n",
            "loss : 0.4068954769573116\n",
            "---BACKWARD PART---\n",
            "iteration :141\n",
            "---FORWARD PART---\n",
            "loss : 0.4542599305656997\n",
            "---BACKWARD PART---\n",
            "iteration :142\n",
            "---FORWARD PART---\n",
            "loss : 0.3565751733964587\n",
            "---BACKWARD PART---\n",
            "iteration :143\n",
            "---FORWARD PART---\n",
            "loss : 0.40100051897889044\n",
            "---BACKWARD PART---\n",
            "iteration :144\n",
            "---FORWARD PART---\n",
            "loss : 0.34257532241949346\n",
            "---BACKWARD PART---\n",
            "iteration :145\n",
            "---FORWARD PART---\n",
            "loss : 0.39696032425342065\n",
            "---BACKWARD PART---\n",
            "iteration :146\n",
            "---FORWARD PART---\n",
            "loss : 0.41269823165668085\n",
            "---BACKWARD PART---\n",
            "iteration :147\n",
            "---FORWARD PART---\n",
            "loss : 0.33563126674693594\n",
            "---BACKWARD PART---\n",
            "iteration :148\n",
            "---FORWARD PART---\n",
            "loss : 0.8151134970746963\n",
            "---BACKWARD PART---\n",
            "iteration :149\n",
            "---FORWARD PART---\n",
            "loss : 0.3777939536886914\n",
            "---BACKWARD PART---\n",
            "iteration :150\n",
            "---FORWARD PART---\n",
            "loss : 0.3585664811070383\n",
            "---BACKWARD PART---\n",
            "iteration :151\n",
            "---FORWARD PART---\n",
            "loss : 0.32118728856527745\n",
            "---BACKWARD PART---\n",
            "iteration :152\n",
            "---FORWARD PART---\n",
            "loss : 0.42616275591019315\n",
            "---BACKWARD PART---\n",
            "iteration :153\n",
            "---FORWARD PART---\n",
            "loss : 0.4893379368140342\n",
            "---BACKWARD PART---\n",
            "iteration :154\n",
            "---FORWARD PART---\n",
            "loss : 0.4296549648485696\n",
            "---BACKWARD PART---\n",
            "iteration :155\n",
            "---FORWARD PART---\n",
            "loss : 0.41709030479599013\n",
            "---BACKWARD PART---\n",
            "iteration :156\n",
            "---FORWARD PART---\n",
            "loss : 0.3146134409321845\n",
            "---BACKWARD PART---\n",
            "iteration :157\n",
            "---FORWARD PART---\n",
            "loss : 0.41781990994517126\n",
            "---BACKWARD PART---\n",
            "iteration :158\n",
            "---FORWARD PART---\n",
            "loss : 0.43465967193239063\n",
            "---BACKWARD PART---\n",
            "iteration :159\n",
            "---FORWARD PART---\n",
            "loss : 0.32136756367485125\n",
            "---BACKWARD PART---\n",
            "iteration :160\n",
            "---FORWARD PART---\n",
            "loss : 0.3380141033667251\n",
            "---BACKWARD PART---\n",
            "iteration :161\n",
            "---FORWARD PART---\n",
            "loss : 0.31013022024211456\n",
            "---BACKWARD PART---\n",
            "iteration :162\n",
            "---FORWARD PART---\n",
            "loss : 0.4103596678642577\n",
            "---BACKWARD PART---\n",
            "iteration :163\n",
            "---FORWARD PART---\n",
            "loss : 0.5831799652967679\n",
            "---BACKWARD PART---\n",
            "iteration :164\n",
            "---FORWARD PART---\n",
            "loss : 0.33995482870713156\n",
            "---BACKWARD PART---\n",
            "iteration :165\n",
            "---FORWARD PART---\n",
            "loss : 0.3915355475314667\n",
            "---BACKWARD PART---\n",
            "iteration :166\n",
            "---FORWARD PART---\n",
            "loss : 0.3053508263444809\n",
            "---BACKWARD PART---\n",
            "iteration :167\n",
            "---FORWARD PART---\n",
            "loss : 0.3074073069261963\n",
            "---BACKWARD PART---\n",
            "iteration :168\n",
            "---FORWARD PART---\n",
            "loss : 0.3868070791427185\n",
            "---BACKWARD PART---\n",
            "iteration :169\n",
            "---FORWARD PART---\n",
            "loss : 0.6204581575761907\n",
            "---BACKWARD PART---\n",
            "iteration :170\n",
            "---FORWARD PART---\n",
            "loss : 0.3273337099873749\n",
            "---BACKWARD PART---\n",
            "iteration :171\n",
            "---FORWARD PART---\n",
            "loss : 0.2975877560436455\n",
            "---BACKWARD PART---\n",
            "iteration :172\n",
            "---FORWARD PART---\n",
            "loss : 1.0473086547916435\n",
            "---BACKWARD PART---\n",
            "iteration :173\n",
            "---FORWARD PART---\n",
            "loss : 0.3080514590697267\n",
            "---BACKWARD PART---\n",
            "iteration :174\n",
            "---FORWARD PART---\n",
            "loss : 0.3398930282113956\n",
            "---BACKWARD PART---\n",
            "iteration :175\n",
            "---FORWARD PART---\n",
            "loss : 0.2974892683152904\n",
            "---BACKWARD PART---\n",
            "iteration :176\n",
            "---FORWARD PART---\n",
            "loss : 0.3747747795059696\n",
            "---BACKWARD PART---\n",
            "iteration :177\n",
            "---FORWARD PART---\n",
            "loss : 0.5172869229178811\n",
            "---BACKWARD PART---\n",
            "iteration :178\n",
            "---FORWARD PART---\n",
            "loss : 0.3038672602039174\n",
            "---BACKWARD PART---\n",
            "iteration :179\n",
            "---FORWARD PART---\n",
            "loss : 0.4253477391004499\n",
            "---BACKWARD PART---\n",
            "iteration :180\n",
            "---FORWARD PART---\n",
            "loss : 0.3613272148903466\n",
            "---BACKWARD PART---\n",
            "iteration :181\n",
            "---FORWARD PART---\n",
            "loss : 0.28589977343024947\n",
            "---BACKWARD PART---\n",
            "iteration :182\n",
            "---FORWARD PART---\n",
            "loss : 0.5539957054415117\n",
            "---BACKWARD PART---\n",
            "iteration :183\n",
            "---FORWARD PART---\n",
            "loss : 0.3649300406597476\n",
            "---BACKWARD PART---\n",
            "iteration :184\n",
            "---FORWARD PART---\n",
            "loss : 0.5202609115207983\n",
            "---BACKWARD PART---\n",
            "iteration :185\n",
            "---FORWARD PART---\n",
            "loss : 0.291717265559054\n",
            "---BACKWARD PART---\n",
            "iteration :186\n",
            "---FORWARD PART---\n",
            "loss : 0.622950150537904\n",
            "---BACKWARD PART---\n",
            "iteration :187\n",
            "---FORWARD PART---\n",
            "loss : 0.2793736173051494\n",
            "---BACKWARD PART---\n",
            "iteration :188\n",
            "---FORWARD PART---\n",
            "loss : 0.27486665561254897\n",
            "---BACKWARD PART---\n",
            "iteration :189\n",
            "---FORWARD PART---\n",
            "loss : 0.27256370484030473\n",
            "---BACKWARD PART---\n",
            "iteration :190\n",
            "---FORWARD PART---\n",
            "loss : 0.2686139381871928\n",
            "---BACKWARD PART---\n",
            "iteration :191\n",
            "---FORWARD PART---\n",
            "loss : 0.3811147009449162\n",
            "---BACKWARD PART---\n",
            "iteration :192\n",
            "---FORWARD PART---\n",
            "loss : 0.8147331135735639\n",
            "---BACKWARD PART---\n",
            "iteration :193\n",
            "---FORWARD PART---\n",
            "loss : 0.8403855808889569\n",
            "---BACKWARD PART---\n",
            "iteration :194\n",
            "---FORWARD PART---\n",
            "loss : 0.37064805603000595\n",
            "---BACKWARD PART---\n",
            "iteration :195\n",
            "---FORWARD PART---\n",
            "loss : 0.26622787074108634\n",
            "---BACKWARD PART---\n",
            "iteration :196\n",
            "---FORWARD PART---\n",
            "loss : 0.26251307358911263\n",
            "---BACKWARD PART---\n",
            "iteration :197\n",
            "---FORWARD PART---\n",
            "loss : 0.4713770324638311\n",
            "---BACKWARD PART---\n",
            "iteration :198\n",
            "---FORWARD PART---\n",
            "loss : 0.8282772509054513\n",
            "---BACKWARD PART---\n",
            "iteration :199\n",
            "---FORWARD PART---\n",
            "loss : 0.25784956954612165\n",
            "---BACKWARD PART---\n",
            "iteration :200\n",
            "---FORWARD PART---\n",
            "loss : 0.3884658675501427\n",
            "---BACKWARD PART---\n",
            "iteration :201\n",
            "---FORWARD PART---\n",
            "loss : 0.3769660143940975\n",
            "---BACKWARD PART---\n",
            "iteration :202\n",
            "---FORWARD PART---\n",
            "loss : 0.46144125389360496\n",
            "---BACKWARD PART---\n",
            "iteration :203\n",
            "---FORWARD PART---\n",
            "loss : 0.44853435655164353\n",
            "---BACKWARD PART---\n",
            "iteration :204\n",
            "---FORWARD PART---\n",
            "loss : 0.4542451065092774\n",
            "---BACKWARD PART---\n",
            "iteration :205\n",
            "---FORWARD PART---\n",
            "loss : 0.36221987169821046\n",
            "---BACKWARD PART---\n",
            "iteration :206\n",
            "---FORWARD PART---\n",
            "loss : 0.26138791427468616\n",
            "---BACKWARD PART---\n",
            "iteration :207\n",
            "---FORWARD PART---\n",
            "loss : 0.25396755152192624\n",
            "---BACKWARD PART---\n",
            "iteration :208\n",
            "---FORWARD PART---\n",
            "loss : 0.2905257175937813\n",
            "---BACKWARD PART---\n",
            "iteration :209\n",
            "---FORWARD PART---\n",
            "loss : 0.3586256221238838\n",
            "---BACKWARD PART---\n",
            "iteration :210\n",
            "---FORWARD PART---\n",
            "loss : 0.35008084363176356\n",
            "---BACKWARD PART---\n",
            "iteration :211\n",
            "---FORWARD PART---\n",
            "loss : 0.25991633214184146\n",
            "---BACKWARD PART---\n",
            "iteration :212\n",
            "---FORWARD PART---\n",
            "loss : 0.623071799750053\n",
            "---BACKWARD PART---\n",
            "iteration :213\n",
            "---FORWARD PART---\n",
            "loss : 0.3739107432176722\n",
            "---BACKWARD PART---\n",
            "iteration :214\n",
            "---FORWARD PART---\n",
            "loss : 0.779477985041844\n",
            "---BACKWARD PART---\n",
            "iteration :215\n",
            "---FORWARD PART---\n",
            "loss : 0.24606169847263493\n",
            "---BACKWARD PART---\n",
            "iteration :216\n",
            "---FORWARD PART---\n",
            "loss : 0.3928172556168703\n",
            "---BACKWARD PART---\n",
            "iteration :217\n",
            "---FORWARD PART---\n",
            "loss : 0.9288662398569104\n",
            "---BACKWARD PART---\n",
            "iteration :218\n",
            "---FORWARD PART---\n",
            "loss : 0.24021386966470334\n",
            "---BACKWARD PART---\n",
            "iteration :219\n",
            "---FORWARD PART---\n",
            "loss : 0.23814772557150907\n",
            "---BACKWARD PART---\n",
            "iteration :220\n",
            "---FORWARD PART---\n",
            "loss : 0.23571348272823264\n",
            "---BACKWARD PART---\n",
            "iteration :221\n",
            "---FORWARD PART---\n",
            "loss : 0.6677272122403778\n",
            "---BACKWARD PART---\n",
            "iteration :222\n",
            "---FORWARD PART---\n",
            "loss : 0.23175958395272023\n",
            "---BACKWARD PART---\n",
            "iteration :223\n",
            "---FORWARD PART---\n",
            "loss : 0.3760571106069168\n",
            "---BACKWARD PART---\n",
            "iteration :224\n",
            "---FORWARD PART---\n",
            "loss : 0.26102618292711105\n",
            "---BACKWARD PART---\n",
            "iteration :225\n",
            "---FORWARD PART---\n",
            "loss : 0.2317882213847437\n",
            "---BACKWARD PART---\n",
            "iteration :226\n",
            "---FORWARD PART---\n",
            "loss : 0.3642248147215825\n",
            "---BACKWARD PART---\n",
            "iteration :227\n",
            "---FORWARD PART---\n",
            "loss : 0.36905899727592817\n",
            "---BACKWARD PART---\n",
            "iteration :228\n",
            "---FORWARD PART---\n",
            "loss : 0.7707749235205179\n",
            "---BACKWARD PART---\n",
            "iteration :229\n",
            "---FORWARD PART---\n",
            "loss : 0.9790871978471323\n",
            "---BACKWARD PART---\n",
            "iteration :230\n",
            "---FORWARD PART---\n",
            "loss : 0.22433063087801633\n",
            "---BACKWARD PART---\n",
            "iteration :231\n",
            "---FORWARD PART---\n",
            "loss : 0.26502835867742813\n",
            "---BACKWARD PART---\n",
            "iteration :232\n",
            "---FORWARD PART---\n",
            "loss : 0.21941123090660167\n",
            "---BACKWARD PART---\n",
            "iteration :233\n",
            "---FORWARD PART---\n",
            "loss : 1.3929019606239599\n",
            "---BACKWARD PART---\n",
            "iteration :234\n",
            "---FORWARD PART---\n",
            "loss : 0.45756990411321957\n",
            "---BACKWARD PART---\n",
            "iteration :235\n",
            "---FORWARD PART---\n",
            "loss : 0.22610192039527727\n",
            "---BACKWARD PART---\n",
            "iteration :236\n",
            "---FORWARD PART---\n",
            "loss : 0.2242235783045962\n",
            "---BACKWARD PART---\n",
            "iteration :237\n",
            "---FORWARD PART---\n",
            "loss : 0.22220740815242432\n",
            "---BACKWARD PART---\n",
            "iteration :238\n",
            "---FORWARD PART---\n",
            "loss : 0.3054667840826446\n",
            "---BACKWARD PART---\n",
            "iteration :239\n",
            "---FORWARD PART---\n",
            "loss : 0.36455211308421787\n",
            "---BACKWARD PART---\n",
            "iteration :240\n",
            "---FORWARD PART---\n",
            "loss : 0.2216062507975115\n",
            "---BACKWARD PART---\n",
            "iteration :241\n",
            "---FORWARD PART---\n",
            "loss : 0.21819850998862175\n",
            "---BACKWARD PART---\n",
            "iteration :242\n",
            "---FORWARD PART---\n",
            "loss : 0.38801938088751575\n",
            "---BACKWARD PART---\n",
            "iteration :243\n",
            "---FORWARD PART---\n",
            "loss : 0.3749391769020666\n",
            "---BACKWARD PART---\n",
            "iteration :244\n",
            "---FORWARD PART---\n",
            "loss : 0.22438648427070657\n",
            "---BACKWARD PART---\n",
            "iteration :245\n",
            "---FORWARD PART---\n",
            "loss : 0.3536090170921842\n",
            "---BACKWARD PART---\n",
            "iteration :246\n",
            "---FORWARD PART---\n",
            "loss : 0.9152860416623286\n",
            "---BACKWARD PART---\n",
            "iteration :247\n",
            "---FORWARD PART---\n",
            "loss : 0.21554528728740402\n",
            "---BACKWARD PART---\n",
            "iteration :248\n",
            "---FORWARD PART---\n",
            "loss : 0.8579449328992292\n",
            "---BACKWARD PART---\n",
            "iteration :249\n",
            "---FORWARD PART---\n",
            "loss : 0.22055350060813345\n",
            "---BACKWARD PART---\n",
            "iteration :250\n",
            "---FORWARD PART---\n",
            "loss : 0.3747132675933553\n",
            "---BACKWARD PART---\n",
            "iteration :251\n",
            "---FORWARD PART---\n",
            "loss : 0.38259207819933394\n",
            "---BACKWARD PART---\n",
            "iteration :252\n",
            "---FORWARD PART---\n",
            "loss : 0.4726965219025629\n",
            "---BACKWARD PART---\n",
            "iteration :253\n",
            "---FORWARD PART---\n",
            "loss : 0.2073716560554532\n",
            "---BACKWARD PART---\n",
            "iteration :254\n",
            "---FORWARD PART---\n",
            "loss : 0.22145740638198236\n",
            "---BACKWARD PART---\n",
            "iteration :255\n",
            "---FORWARD PART---\n",
            "loss : 0.4736136634799648\n",
            "---BACKWARD PART---\n",
            "iteration :256\n",
            "---FORWARD PART---\n",
            "loss : 0.2972206161030561\n",
            "---BACKWARD PART---\n",
            "iteration :257\n",
            "---FORWARD PART---\n",
            "loss : 0.20392966009163205\n",
            "---BACKWARD PART---\n",
            "iteration :258\n",
            "---FORWARD PART---\n",
            "loss : 0.40608320730160846\n",
            "---BACKWARD PART---\n",
            "iteration :259\n",
            "---FORWARD PART---\n",
            "loss : 0.3507403524875129\n",
            "---BACKWARD PART---\n",
            "iteration :260\n",
            "---FORWARD PART---\n",
            "loss : 0.38259233716439073\n",
            "---BACKWARD PART---\n",
            "iteration :261\n",
            "---FORWARD PART---\n",
            "loss : 0.20506575385887937\n",
            "---BACKWARD PART---\n",
            "iteration :262\n",
            "---FORWARD PART---\n",
            "loss : 0.20503404907142456\n",
            "---BACKWARD PART---\n",
            "iteration :263\n",
            "---FORWARD PART---\n",
            "loss : 0.3483194104536474\n",
            "---BACKWARD PART---\n",
            "iteration :264\n",
            "---FORWARD PART---\n",
            "loss : 0.32705386161603817\n",
            "---BACKWARD PART---\n",
            "iteration :265\n",
            "---FORWARD PART---\n",
            "loss : 0.20754250605968677\n",
            "---BACKWARD PART---\n",
            "iteration :266\n",
            "---FORWARD PART---\n",
            "loss : 0.20530688959866134\n",
            "---BACKWARD PART---\n",
            "iteration :267\n",
            "---FORWARD PART---\n",
            "loss : 0.2021191786758548\n",
            "---BACKWARD PART---\n",
            "iteration :268\n",
            "---FORWARD PART---\n",
            "loss : 0.1993772363863145\n",
            "---BACKWARD PART---\n",
            "iteration :269\n",
            "---FORWARD PART---\n",
            "loss : 0.20251007818328795\n",
            "---BACKWARD PART---\n",
            "iteration :270\n",
            "---FORWARD PART---\n",
            "loss : 0.1964256812360566\n",
            "---BACKWARD PART---\n",
            "iteration :271\n",
            "---FORWARD PART---\n",
            "loss : 0.348910298200659\n",
            "---BACKWARD PART---\n",
            "iteration :272\n",
            "---FORWARD PART---\n",
            "loss : 0.3084323111488114\n",
            "---BACKWARD PART---\n",
            "iteration :273\n",
            "---FORWARD PART---\n",
            "loss : 0.3210946914950321\n",
            "---BACKWARD PART---\n",
            "iteration :274\n",
            "---FORWARD PART---\n",
            "loss : 0.2434066741137209\n",
            "---BACKWARD PART---\n",
            "iteration :275\n",
            "---FORWARD PART---\n",
            "loss : 0.19379914990732214\n",
            "---BACKWARD PART---\n",
            "iteration :276\n",
            "---FORWARD PART---\n",
            "loss : 0.20868669155036915\n",
            "---BACKWARD PART---\n",
            "iteration :277\n",
            "---FORWARD PART---\n",
            "loss : 0.19289638567190853\n",
            "---BACKWARD PART---\n",
            "iteration :278\n",
            "---FORWARD PART---\n",
            "loss : 0.32052180746911424\n",
            "---BACKWARD PART---\n",
            "iteration :279\n",
            "---FORWARD PART---\n",
            "loss : 0.5456608302642376\n",
            "---BACKWARD PART---\n",
            "iteration :280\n",
            "---FORWARD PART---\n",
            "loss : 0.33108224034447314\n",
            "---BACKWARD PART---\n",
            "iteration :281\n",
            "---FORWARD PART---\n",
            "loss : 0.186243039499843\n",
            "---BACKWARD PART---\n",
            "iteration :282\n",
            "---FORWARD PART---\n",
            "loss : 0.32085594828965475\n",
            "---BACKWARD PART---\n",
            "iteration :283\n",
            "---FORWARD PART---\n",
            "loss : 1.120889609965646\n",
            "---BACKWARD PART---\n",
            "iteration :284\n",
            "---FORWARD PART---\n",
            "loss : 0.3291086697538844\n",
            "---BACKWARD PART---\n",
            "iteration :285\n",
            "---FORWARD PART---\n",
            "loss : 0.3608453152478008\n",
            "---BACKWARD PART---\n",
            "iteration :286\n",
            "---FORWARD PART---\n",
            "loss : 0.3146091749255587\n",
            "---BACKWARD PART---\n",
            "iteration :287\n",
            "---FORWARD PART---\n",
            "loss : 0.19572388438897262\n",
            "---BACKWARD PART---\n",
            "iteration :288\n",
            "---FORWARD PART---\n",
            "loss : 0.32035288421569574\n",
            "---BACKWARD PART---\n",
            "iteration :289\n",
            "---FORWARD PART---\n",
            "loss : 0.3116577044949807\n",
            "---BACKWARD PART---\n",
            "iteration :290\n",
            "---FORWARD PART---\n",
            "loss : 0.29430596192779396\n",
            "---BACKWARD PART---\n",
            "iteration :291\n",
            "---FORWARD PART---\n",
            "loss : 0.20014955086832703\n",
            "---BACKWARD PART---\n",
            "iteration :292\n",
            "---FORWARD PART---\n",
            "loss : 0.32158502686120793\n",
            "---BACKWARD PART---\n",
            "iteration :293\n",
            "---FORWARD PART---\n",
            "loss : 0.1922212719605451\n",
            "---BACKWARD PART---\n",
            "iteration :294\n",
            "---FORWARD PART---\n",
            "loss : 0.2878729554605148\n",
            "---BACKWARD PART---\n",
            "iteration :295\n",
            "---FORWARD PART---\n",
            "loss : 0.1905046753411031\n",
            "---BACKWARD PART---\n",
            "iteration :296\n",
            "---FORWARD PART---\n",
            "loss : 1.2674746035022209\n",
            "---BACKWARD PART---\n",
            "iteration :297\n",
            "---FORWARD PART---\n",
            "loss : 0.1951884990899836\n",
            "---BACKWARD PART---\n",
            "iteration :298\n",
            "---FORWARD PART---\n",
            "loss : 0.19031341360973322\n",
            "---BACKWARD PART---\n",
            "iteration :299\n",
            "---FORWARD PART---\n",
            "loss : 0.3121468586483248\n",
            "---BACKWARD PART---\n",
            "iteration :300\n",
            "---FORWARD PART---\n",
            "loss : 0.1838713560054087\n",
            "---BACKWARD PART---\n",
            "iteration :301\n",
            "---FORWARD PART---\n",
            "loss : 0.18436519470361637\n",
            "---BACKWARD PART---\n",
            "iteration :302\n",
            "---FORWARD PART---\n",
            "loss : 0.17910793356426655\n",
            "---BACKWARD PART---\n",
            "iteration :303\n",
            "---FORWARD PART---\n",
            "loss : 0.18435555932795755\n",
            "---BACKWARD PART---\n",
            "iteration :304\n",
            "---FORWARD PART---\n",
            "loss : 0.17741983981856693\n",
            "---BACKWARD PART---\n",
            "iteration :305\n",
            "---FORWARD PART---\n",
            "loss : 0.18067466167576884\n",
            "---BACKWARD PART---\n",
            "iteration :306\n",
            "---FORWARD PART---\n",
            "loss : 0.1884815747035241\n",
            "---BACKWARD PART---\n",
            "iteration :307\n",
            "---FORWARD PART---\n",
            "loss : 0.30061833872999394\n",
            "---BACKWARD PART---\n",
            "iteration :308\n",
            "---FORWARD PART---\n",
            "loss : 0.17440291480234948\n",
            "---BACKWARD PART---\n",
            "iteration :309\n",
            "---FORWARD PART---\n",
            "loss : 0.3673452236449716\n",
            "---BACKWARD PART---\n",
            "iteration :310\n",
            "---FORWARD PART---\n",
            "loss : 0.18924367604021974\n",
            "---BACKWARD PART---\n",
            "iteration :311\n",
            "---FORWARD PART---\n",
            "loss : 0.1735990246830959\n",
            "---BACKWARD PART---\n",
            "iteration :312\n",
            "---FORWARD PART---\n",
            "loss : 0.8645872178028666\n",
            "---BACKWARD PART---\n",
            "iteration :313\n",
            "---FORWARD PART---\n",
            "loss : 0.341374509777662\n",
            "---BACKWARD PART---\n",
            "iteration :314\n",
            "---FORWARD PART---\n",
            "loss : 0.29745212902262996\n",
            "---BACKWARD PART---\n",
            "iteration :315\n",
            "---FORWARD PART---\n",
            "loss : 0.1708067337052676\n",
            "---BACKWARD PART---\n",
            "iteration :316\n",
            "---FORWARD PART---\n",
            "loss : 0.16911429417788384\n",
            "---BACKWARD PART---\n",
            "iteration :317\n",
            "---FORWARD PART---\n",
            "loss : 0.3607190934254382\n",
            "---BACKWARD PART---\n",
            "iteration :318\n",
            "---FORWARD PART---\n",
            "loss : 0.2827212178972263\n",
            "---BACKWARD PART---\n",
            "iteration :319\n",
            "---FORWARD PART---\n",
            "loss : 0.28656536879556566\n",
            "---BACKWARD PART---\n",
            "iteration :320\n",
            "---FORWARD PART---\n",
            "loss : 0.17102124402833133\n",
            "---BACKWARD PART---\n",
            "iteration :321\n",
            "---FORWARD PART---\n",
            "loss : 0.16995422237009164\n",
            "---BACKWARD PART---\n",
            "iteration :322\n",
            "---FORWARD PART---\n",
            "loss : 0.35407463778164927\n",
            "---BACKWARD PART---\n",
            "iteration :323\n",
            "---FORWARD PART---\n",
            "loss : 0.1772641978832556\n",
            "---BACKWARD PART---\n",
            "iteration :324\n",
            "---FORWARD PART---\n",
            "loss : 0.16581631861758642\n",
            "---BACKWARD PART---\n",
            "iteration :325\n",
            "---FORWARD PART---\n",
            "loss : 0.16530563138095347\n",
            "---BACKWARD PART---\n",
            "iteration :326\n",
            "---FORWARD PART---\n",
            "loss : 0.16357197369496457\n",
            "---BACKWARD PART---\n",
            "iteration :327\n",
            "---FORWARD PART---\n",
            "loss : 0.18530198911753853\n",
            "---BACKWARD PART---\n",
            "iteration :328\n",
            "---FORWARD PART---\n",
            "loss : 0.2898753831668618\n",
            "---BACKWARD PART---\n",
            "iteration :329\n",
            "---FORWARD PART---\n",
            "loss : 0.1622765820715193\n",
            "---BACKWARD PART---\n",
            "iteration :330\n",
            "---FORWARD PART---\n",
            "loss : 0.2731658486440791\n",
            "---BACKWARD PART---\n",
            "iteration :331\n",
            "---FORWARD PART---\n",
            "loss : 0.3344551094034122\n",
            "---BACKWARD PART---\n",
            "iteration :332\n",
            "---FORWARD PART---\n",
            "loss : 0.16348859446084887\n",
            "---BACKWARD PART---\n",
            "iteration :333\n",
            "---FORWARD PART---\n",
            "loss : 0.40932530790217947\n",
            "---BACKWARD PART---\n",
            "iteration :334\n",
            "---FORWARD PART---\n",
            "loss : 0.2646357992223096\n",
            "---BACKWARD PART---\n",
            "iteration :335\n",
            "---FORWARD PART---\n",
            "loss : 0.16520399452060933\n",
            "---BACKWARD PART---\n",
            "iteration :336\n",
            "---FORWARD PART---\n",
            "loss : 0.34916824824971976\n",
            "---BACKWARD PART---\n",
            "iteration :337\n",
            "---FORWARD PART---\n",
            "loss : 0.2807131134536529\n",
            "---BACKWARD PART---\n",
            "iteration :338\n",
            "---FORWARD PART---\n",
            "loss : 0.16282617336147923\n",
            "---BACKWARD PART---\n",
            "iteration :339\n",
            "---FORWARD PART---\n",
            "loss : 0.17420220748131715\n",
            "---BACKWARD PART---\n",
            "iteration :340\n",
            "---FORWARD PART---\n",
            "loss : 0.1600982628032035\n",
            "---BACKWARD PART---\n",
            "iteration :341\n",
            "---FORWARD PART---\n",
            "loss : 0.2579136974854414\n",
            "---BACKWARD PART---\n",
            "iteration :342\n",
            "---FORWARD PART---\n",
            "loss : 0.21024719144232756\n",
            "---BACKWARD PART---\n",
            "iteration :343\n",
            "---FORWARD PART---\n",
            "loss : 0.15893279966022908\n",
            "---BACKWARD PART---\n",
            "iteration :344\n",
            "---FORWARD PART---\n",
            "loss : 0.25317458379591923\n",
            "---BACKWARD PART---\n",
            "iteration :345\n",
            "---FORWARD PART---\n",
            "loss : 0.1580773113412725\n",
            "---BACKWARD PART---\n",
            "iteration :346\n",
            "---FORWARD PART---\n",
            "loss : 0.1570886531346064\n",
            "---BACKWARD PART---\n",
            "iteration :347\n",
            "---FORWARD PART---\n",
            "loss : 0.23110562518324076\n",
            "---BACKWARD PART---\n",
            "iteration :348\n",
            "---FORWARD PART---\n",
            "loss : 0.1546638750017438\n",
            "---BACKWARD PART---\n",
            "iteration :349\n",
            "---FORWARD PART---\n",
            "loss : 0.2674364170770142\n",
            "---BACKWARD PART---\n",
            "iteration :350\n",
            "---FORWARD PART---\n",
            "loss : 0.2558406707876104\n",
            "---BACKWARD PART---\n",
            "iteration :351\n",
            "---FORWARD PART---\n",
            "loss : 0.23914782487499026\n",
            "---BACKWARD PART---\n",
            "iteration :352\n",
            "---FORWARD PART---\n",
            "loss : 0.3736918655929886\n",
            "---BACKWARD PART---\n",
            "iteration :353\n",
            "---FORWARD PART---\n",
            "loss : 0.15409109778609142\n",
            "---BACKWARD PART---\n",
            "iteration :354\n",
            "---FORWARD PART---\n",
            "loss : 0.2420849749402787\n",
            "---BACKWARD PART---\n",
            "iteration :355\n",
            "---FORWARD PART---\n",
            "loss : 0.15361077660138897\n",
            "---BACKWARD PART---\n",
            "iteration :356\n",
            "---FORWARD PART---\n",
            "loss : 0.23536139594115127\n",
            "---BACKWARD PART---\n",
            "iteration :357\n",
            "---FORWARD PART---\n",
            "loss : 0.23187446524877806\n",
            "---BACKWARD PART---\n",
            "iteration :358\n",
            "---FORWARD PART---\n",
            "loss : 0.15494625285322572\n",
            "---BACKWARD PART---\n",
            "iteration :359\n",
            "---FORWARD PART---\n",
            "loss : 0.15386128531201476\n",
            "---BACKWARD PART---\n",
            "iteration :360\n",
            "---FORWARD PART---\n",
            "loss : 0.22802933770088624\n",
            "---BACKWARD PART---\n",
            "iteration :361\n",
            "---FORWARD PART---\n",
            "loss : 0.22557322831178284\n",
            "---BACKWARD PART---\n",
            "iteration :362\n",
            "---FORWARD PART---\n",
            "loss : 0.46123351581464983\n",
            "---BACKWARD PART---\n",
            "iteration :363\n",
            "---FORWARD PART---\n",
            "loss : 0.22292850512358914\n",
            "---BACKWARD PART---\n",
            "iteration :364\n",
            "---FORWARD PART---\n",
            "loss : 0.42858578716408163\n",
            "---BACKWARD PART---\n",
            "iteration :365\n",
            "---FORWARD PART---\n",
            "loss : 0.15325984420957284\n",
            "---BACKWARD PART---\n",
            "iteration :366\n",
            "---FORWARD PART---\n",
            "loss : 0.26987140926022557\n",
            "---BACKWARD PART---\n",
            "iteration :367\n",
            "---FORWARD PART---\n",
            "loss : 0.3225376895734893\n",
            "---BACKWARD PART---\n",
            "iteration :368\n",
            "---FORWARD PART---\n",
            "loss : 0.2313021372229227\n",
            "---BACKWARD PART---\n",
            "iteration :369\n",
            "---FORWARD PART---\n",
            "loss : 0.22257187014296526\n",
            "---BACKWARD PART---\n",
            "iteration :370\n",
            "---FORWARD PART---\n",
            "loss : 0.15040488782633474\n",
            "---BACKWARD PART---\n",
            "iteration :371\n",
            "---FORWARD PART---\n",
            "loss : 0.6250509406946381\n",
            "---BACKWARD PART---\n",
            "iteration :372\n",
            "---FORWARD PART---\n",
            "loss : 0.5990990150522673\n",
            "---BACKWARD PART---\n",
            "iteration :373\n",
            "---FORWARD PART---\n",
            "loss : 0.2894012867908607\n",
            "---BACKWARD PART---\n",
            "iteration :374\n",
            "---FORWARD PART---\n",
            "loss : 0.15843106442003826\n",
            "---BACKWARD PART---\n",
            "iteration :375\n",
            "---FORWARD PART---\n",
            "loss : 0.14985397610694806\n",
            "---BACKWARD PART---\n",
            "iteration :376\n",
            "---FORWARD PART---\n",
            "loss : 0.14741875898024837\n",
            "---BACKWARD PART---\n",
            "iteration :377\n",
            "---FORWARD PART---\n",
            "loss : 0.23020259224074\n",
            "---BACKWARD PART---\n",
            "iteration :378\n",
            "---FORWARD PART---\n",
            "loss : 0.2391084443145274\n",
            "---BACKWARD PART---\n",
            "iteration :379\n",
            "---FORWARD PART---\n",
            "loss : 0.16216718425978158\n",
            "---BACKWARD PART---\n",
            "iteration :380\n",
            "---FORWARD PART---\n",
            "loss : 0.21158950073456814\n",
            "---BACKWARD PART---\n",
            "iteration :381\n",
            "---FORWARD PART---\n",
            "loss : 0.2199448172293969\n",
            "---BACKWARD PART---\n",
            "iteration :382\n",
            "---FORWARD PART---\n",
            "loss : 0.32211745181984824\n",
            "---BACKWARD PART---\n",
            "iteration :383\n",
            "---FORWARD PART---\n",
            "loss : 1.2363426872042165\n",
            "---BACKWARD PART---\n",
            "iteration :384\n",
            "---FORWARD PART---\n",
            "loss : 0.5428359386686885\n",
            "---BACKWARD PART---\n",
            "iteration :385\n",
            "---FORWARD PART---\n",
            "loss : 0.2010009673234848\n",
            "---BACKWARD PART---\n",
            "iteration :386\n",
            "---FORWARD PART---\n",
            "loss : 0.15155388012509574\n",
            "---BACKWARD PART---\n",
            "iteration :387\n",
            "---FORWARD PART---\n",
            "loss : 0.3436301317550076\n",
            "---BACKWARD PART---\n",
            "iteration :388\n",
            "---FORWARD PART---\n",
            "loss : 0.4220441425065282\n",
            "---BACKWARD PART---\n",
            "iteration :389\n",
            "---FORWARD PART---\n",
            "loss : 0.15284114757641457\n",
            "---BACKWARD PART---\n",
            "iteration :390\n",
            "---FORWARD PART---\n",
            "loss : 0.15061708511281333\n",
            "---BACKWARD PART---\n",
            "iteration :391\n",
            "---FORWARD PART---\n",
            "loss : 0.19841038317861076\n",
            "---BACKWARD PART---\n",
            "iteration :392\n",
            "---FORWARD PART---\n",
            "loss : 0.1897029910585968\n",
            "---BACKWARD PART---\n",
            "iteration :393\n",
            "---FORWARD PART---\n",
            "loss : 0.14966873744371917\n",
            "---BACKWARD PART---\n",
            "iteration :394\n",
            "---FORWARD PART---\n",
            "loss : 0.15055808643903004\n",
            "---BACKWARD PART---\n",
            "iteration :395\n",
            "---FORWARD PART---\n",
            "loss : 0.21369901111049466\n",
            "---BACKWARD PART---\n",
            "iteration :396\n",
            "---FORWARD PART---\n",
            "loss : 0.2661413903479877\n",
            "---BACKWARD PART---\n",
            "iteration :397\n",
            "---FORWARD PART---\n",
            "loss : 0.3028738395347769\n",
            "---BACKWARD PART---\n",
            "iteration :398\n",
            "---FORWARD PART---\n",
            "loss : 0.19564106621631772\n",
            "---BACKWARD PART---\n",
            "iteration :399\n",
            "---FORWARD PART---\n",
            "loss : 0.15356556359791101\n",
            "---BACKWARD PART---\n",
            "iteration :400\n",
            "---FORWARD PART---\n",
            "loss : 0.19330427081155885\n",
            "---BACKWARD PART---\n",
            "iteration :401\n",
            "---FORWARD PART---\n",
            "loss : 0.1911266525575074\n",
            "---BACKWARD PART---\n",
            "iteration :402\n",
            "---FORWARD PART---\n",
            "loss : 0.1507961130724567\n",
            "---BACKWARD PART---\n",
            "iteration :403\n",
            "---FORWARD PART---\n",
            "loss : 0.18850528434109168\n",
            "---BACKWARD PART---\n",
            "iteration :404\n",
            "---FORWARD PART---\n",
            "loss : 0.1872899998256775\n",
            "---BACKWARD PART---\n",
            "iteration :405\n",
            "---FORWARD PART---\n",
            "loss : 0.1829335351450448\n",
            "---BACKWARD PART---\n",
            "iteration :406\n",
            "---FORWARD PART---\n",
            "loss : 0.25037049887169294\n",
            "---BACKWARD PART---\n",
            "iteration :407\n",
            "---FORWARD PART---\n",
            "loss : 1.1150831573732964\n",
            "---BACKWARD PART---\n",
            "iteration :408\n",
            "---FORWARD PART---\n",
            "loss : 0.41690094345318346\n",
            "---BACKWARD PART---\n",
            "iteration :409\n",
            "---FORWARD PART---\n",
            "loss : 0.16230161407649116\n",
            "---BACKWARD PART---\n",
            "iteration :410\n",
            "---FORWARD PART---\n",
            "loss : 1.2568297128131474\n",
            "---BACKWARD PART---\n",
            "iteration :411\n",
            "---FORWARD PART---\n",
            "loss : 0.5030918334062764\n",
            "---BACKWARD PART---\n",
            "iteration :412\n",
            "---FORWARD PART---\n",
            "loss : 0.3050854752625504\n",
            "---BACKWARD PART---\n",
            "iteration :413\n",
            "---FORWARD PART---\n",
            "loss : 0.19770677019979926\n",
            "---BACKWARD PART---\n",
            "iteration :414\n",
            "---FORWARD PART---\n",
            "loss : 0.44202902719757925\n",
            "---BACKWARD PART---\n",
            "iteration :415\n",
            "---FORWARD PART---\n",
            "loss : 0.3099723466834074\n",
            "---BACKWARD PART---\n",
            "iteration :416\n",
            "---FORWARD PART---\n",
            "loss : 0.40959251431334853\n",
            "---BACKWARD PART---\n",
            "iteration :417\n",
            "---FORWARD PART---\n",
            "loss : 0.5161110394901977\n",
            "---BACKWARD PART---\n",
            "iteration :418\n",
            "---FORWARD PART---\n",
            "loss : 0.1439171783619689\n",
            "---BACKWARD PART---\n",
            "iteration :419\n",
            "---FORWARD PART---\n",
            "loss : 0.3037628858534892\n",
            "---BACKWARD PART---\n",
            "iteration :420\n",
            "---FORWARD PART---\n",
            "loss : 0.31230679961918867\n",
            "---BACKWARD PART---\n",
            "iteration :421\n",
            "---FORWARD PART---\n",
            "loss : 0.22973408417548216\n",
            "---BACKWARD PART---\n",
            "iteration :422\n",
            "---FORWARD PART---\n",
            "loss : 0.1415397838545704\n",
            "---BACKWARD PART---\n",
            "iteration :423\n",
            "---FORWARD PART---\n",
            "loss : 0.17569728776788385\n",
            "---BACKWARD PART---\n",
            "iteration :424\n",
            "---FORWARD PART---\n",
            "loss : 0.5975525547731393\n",
            "---BACKWARD PART---\n",
            "iteration :425\n",
            "---FORWARD PART---\n",
            "loss : 0.34367196307128944\n",
            "---BACKWARD PART---\n",
            "iteration :426\n",
            "---FORWARD PART---\n",
            "loss : 0.19284300783758773\n",
            "---BACKWARD PART---\n",
            "iteration :427\n",
            "---FORWARD PART---\n",
            "loss : 0.14239500865103685\n",
            "---BACKWARD PART---\n",
            "iteration :428\n",
            "---FORWARD PART---\n",
            "loss : 0.20059471579213922\n",
            "---BACKWARD PART---\n",
            "iteration :429\n",
            "---FORWARD PART---\n",
            "loss : 0.14148785322149074\n",
            "---BACKWARD PART---\n",
            "iteration :430\n",
            "---FORWARD PART---\n",
            "loss : 0.18399381895953315\n",
            "---BACKWARD PART---\n",
            "iteration :431\n",
            "---FORWARD PART---\n",
            "loss : 0.23050018450242984\n",
            "---BACKWARD PART---\n",
            "iteration :432\n",
            "---FORWARD PART---\n",
            "loss : 0.14164583918401424\n",
            "---BACKWARD PART---\n",
            "iteration :433\n",
            "---FORWARD PART---\n",
            "loss : 0.2686388078665803\n",
            "---BACKWARD PART---\n",
            "iteration :434\n",
            "---FORWARD PART---\n",
            "loss : 0.15087756230492508\n",
            "---BACKWARD PART---\n",
            "iteration :435\n",
            "---FORWARD PART---\n",
            "loss : 0.1392427103166723\n",
            "---BACKWARD PART---\n",
            "iteration :436\n",
            "---FORWARD PART---\n",
            "loss : 0.275537452468449\n",
            "---BACKWARD PART---\n",
            "iteration :437\n",
            "---FORWARD PART---\n",
            "loss : 0.17704500044956897\n",
            "---BACKWARD PART---\n",
            "iteration :438\n",
            "---FORWARD PART---\n",
            "loss : 0.1662101177792222\n",
            "---BACKWARD PART---\n",
            "iteration :439\n",
            "---FORWARD PART---\n",
            "loss : 0.16392018604534966\n",
            "---BACKWARD PART---\n",
            "iteration :440\n",
            "---FORWARD PART---\n",
            "loss : 0.19202188330777023\n",
            "---BACKWARD PART---\n",
            "iteration :441\n",
            "---FORWARD PART---\n",
            "loss : 0.1361663152654417\n",
            "---BACKWARD PART---\n",
            "iteration :442\n",
            "---FORWARD PART---\n",
            "loss : 0.13928462129777222\n",
            "---BACKWARD PART---\n",
            "iteration :443\n",
            "---FORWARD PART---\n",
            "loss : 0.22836436647257505\n",
            "---BACKWARD PART---\n",
            "iteration :444\n",
            "---FORWARD PART---\n",
            "loss : 0.16955526862601739\n",
            "---BACKWARD PART---\n",
            "iteration :445\n",
            "---FORWARD PART---\n",
            "loss : 0.15915172524172091\n",
            "---BACKWARD PART---\n",
            "iteration :446\n",
            "---FORWARD PART---\n",
            "loss : 0.24934520557856313\n",
            "---BACKWARD PART---\n",
            "iteration :447\n",
            "---FORWARD PART---\n",
            "loss : 0.16742663782735293\n",
            "---BACKWARD PART---\n",
            "iteration :448\n",
            "---FORWARD PART---\n",
            "loss : 0.1603397598143949\n",
            "---BACKWARD PART---\n",
            "iteration :449\n",
            "---FORWARD PART---\n",
            "loss : 0.21376260181710705\n",
            "---BACKWARD PART---\n",
            "iteration :450\n",
            "---FORWARD PART---\n",
            "loss : 0.17512624589652653\n",
            "---BACKWARD PART---\n",
            "iteration :451\n",
            "---FORWARD PART---\n",
            "loss : 0.15422210878429757\n",
            "---BACKWARD PART---\n",
            "iteration :452\n",
            "---FORWARD PART---\n",
            "loss : 0.18045843205403728\n",
            "---BACKWARD PART---\n",
            "iteration :453\n",
            "---FORWARD PART---\n",
            "loss : 0.1529618038555332\n",
            "---BACKWARD PART---\n",
            "iteration :454\n",
            "---FORWARD PART---\n",
            "loss : 0.231129516134349\n",
            "---BACKWARD PART---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hng1TkLJgDk5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "9a8623c0-5105-4212-a387-beacb8e2c25b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ann_test(\"sigmoid\") # Testing the network"
      ],
      "metadata": {
        "id": "eHYaNOZggF64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4db3c4-0328-4b4c-aba3-eac402363eb7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration :0\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :1\n",
            "---FORWARD PART---\n",
            "iteration :2\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :3\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :4\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :5\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :6\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :7\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :8\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :9\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :10\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :11\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :12\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :13\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :14\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :15\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :16\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :17\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :18\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :19\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :20\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :21\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :22\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :23\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :24\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :25\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :26\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :27\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :28\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :29\n",
            "---FORWARD PART---\n",
            "iteration :30\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :31\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :32\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :33\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :34\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :35\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :36\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :37\n",
            "---FORWARD PART---\n",
            "iteration :38\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :39\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :40\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :41\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :42\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :43\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :44\n",
            "---FORWARD PART---\n",
            "iteration :45\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :46\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :47\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :48\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :49\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :50\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :51\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :52\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :53\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :54\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :55\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :56\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :57\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :58\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :59\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :60\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :61\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :62\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :63\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :64\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :65\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :66\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :67\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :68\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :69\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :70\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :71\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :72\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :73\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :74\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :75\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :76\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :77\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :78\n",
            "---FORWARD PART---\n",
            "iteration :79\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :80\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :81\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :82\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :83\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :84\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :85\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :86\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :87\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :88\n",
            "---FORWARD PART---\n",
            "iteration :89\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :90\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :91\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :92\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :93\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :94\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :95\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :96\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :97\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :98\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :99\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :100\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :101\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :102\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :103\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :104\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :105\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :106\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :107\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :108\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :109\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :110\n",
            "---FORWARD PART---\n",
            "iteration :111\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :112\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "iteration :113\n",
            "---FORWARD PART---\n",
            "example successfully classed\n",
            "accuracy : 0.9385964912280702\n"
          ]
        }
      ]
    }
  ]
}